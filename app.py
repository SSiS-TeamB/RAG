import time
import streamlit as st
from chromaVectorStore import ChromaVectorStore
from rag import RAGPipeline
from concurrent.futures import ThreadPoolExecutor, as_completed
from streamlit.runtime.scriptrunner import add_script_run_ctx
from PIL import Image

### MultiThreadë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ì‹¤í–‰ìš”ì†Œë§Œ ë”°ë¡œ ëºë‹¤.
## RAGPipeline.invoke, RAGPipeline.retrieve -> ì‹¤í–‰ í›„ time check
def run_pipeline_task(query, task_func):
    """ MultiThreading API calling Executor """
    start_time = time.time()
    try:
        result = task_func(query)
    except Exception as e:
        result = str(e)
    elapsed_time = time.time() - start_time
    # print(elapsed_time)
    return result, elapsed_time

def page_config():
    """ Execute page config """
    st.set_page_config(
    page_title="Welfare Search Serviece",
    page_icon="âœ¨",
    layout="centered",
    initial_sidebar_state="collapsed",
    )
    #Title
    title = '''<h1 style='text-align: center'>ë³µì§€ ì •ë³´ ê²€ìƒ‰ ì„œë¹„ìŠ¤</h1><br>
    <center>ë‚˜ì—ê²Œ ë”± ë§ëŠ” ë³µì§€ ì •ë³´<br>
    ì´ì œëŠ” ëˆ„êµ¬ë‚˜ ì‰½ê²Œ, ë‚´ ë§ˆìŒëŒ€ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆì–´ìš”!</center><br>
    '''
    st.markdown(title, unsafe_allow_html=True)
    #Image
    img1, img2 = st.columns(2)
    with img1:
        img_ssis = Image.open('image/ssis_logo.png')
        img1.image(img_ssis, use_column_width=True)
    with img2:
        img_BL = Image.open('image/bigleader_logo.png')
        img2.image(img_BL, use_column_width=True)
    st.subheader("", divider='blue')
    #Query example for user
    st.subheader("ğŸ“Œì´ë ‡ê²Œ ê²€ìƒ‰í•´ë³´ì„¸ìš”!")
    st.info('''
            ì˜ˆì‹œ)\n
            â“"ëŒ€í•™ìƒì¸ë° í•™ìê¸ˆ ëŒ€ì¶œ ë§ê³ ë„ ê¸ˆì „ì ì¸ ì§€ì›ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆë‚˜ìš”?"\n
            â“"ë…¸ì¸ ë¶„ë“¤ì´ ë¬¸í™”ìƒí™œì„ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ê³µê°„ìœ¼ë¡œ, ì–´ë–¤ ì‹œì„¤ì´ ìˆëŠ”ì§€ ê¶ê¸ˆí•´ìš”."\n
            â“"ìš°ë¦¬ ì§‘ì´ ìµœê·¼ í™”ì¬ë¡œ ì¸í•´ ì‚´ ê³³ì„ ìƒì—ˆì–´ìš”. ê¸´ê¸‰í•˜ê²Œ ì§€ì›ë°›ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ê¹Œìš”?"
            ''')

    return

def vectorstore_config():
    """ VectorStore Config """
    # Settings for semantic_search using vectorstores of langchain
    collection_name = "wf_schema_split"
    persist_directory = "workspace/chroma_storage"            

    vectorstore = ChromaVectorStore(**{
    "collection_name":collection_name, 
    "persist_directory":persist_directory,
    "collection_metadata" : {"hnsw:space":"cosine"}
    })
    return vectorstore

def main() :
    #load page config
    page_config()
    ##### LLM container
    llm_selector = st.container()
    with llm_selector:
        st.write("")
        st.subheader("âš™ï¸ë‹µë³€ ìƒì„± AI ëª¨ë“œ ì„¤ì •")
        option = st.selectbox(
            "ì •í™•í•œ ë‹µë³€ ìƒì„±ì€ ì¡°ê¸ˆ ëŠë¦´ ìˆ˜ ìˆì–´ìš”.",
            ('ë¹ ë¥¸ ìƒì„±', 'ì •í™•í•œ ìƒì„±'),
            label_visibility="visible",
        )
        # option_speed, option_accuracy = st.columns([0.2, 0.8])
        # gpt_3_5 = option_speed.button("ë¹ ë¥¸ ê²€ìƒ‰")
        # gpt_4 = option_accuracy.button("ì •í™•í•œ ê²€ìƒ‰")
        if option == 'ë¹ ë¥¸ ìƒì„±':
            model = "gpt-3.5-turbo-1106"
            search_name = "ë¹ ë¥¸ ìƒì„±"
        else:
            model = "gpt-4-1106-preview"
            search_name = "ì •í™•í•œ ìƒì„±"
    ##### QUERY CONFIG
    query_container = st.container()
    with query_container:
        st.subheader("ğŸ”ê²€ìƒ‰")
        query = st.text_input("Search Bar", placeholder="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.", label_visibility="hidden")
        search_button = st.button(search_name, use_container_width=True)
        st.divider()
    

    # í•„í„°ë§
    category_list = ["01 ìƒê³„ ì§€ì›", "02 ì·¨ì—… ì§€ì›", "03 ì„ì‹ Â·ë³´ìœ¡ ì§€ì›", "04 ì²­ì†Œë…„Â·ì²­ë…„ ì§€ì›", "05 ë³´ê±´ì˜ë£Œ ì§€ì›", "06 ë…¸ë ¹ì¸µ ì§€ì›", "07 ì¥ì• ì¸ ì§€ì›", "08 ë³´í›ˆëŒ€ìƒì ì§€ì›", "09 ë²•ë¥ Â·ê¸ˆìœµ ë³µì§€ ì§€ì›", "10 ê¸°íƒ€ ìœ„ê¸°ë³„Â·ìƒí™©ë³„ ì§€ì›"]
    filter_len = len(category_list)
    chk_idxes = [True]*filter_len
    meta_filter = st.container()
    with meta_filter:
        st.subheader("âš™ï¸í•„í„° ì ìš©")
        with st.expander("ì£¼ì œ ì„ íƒ"):
            for i in range(filter_len):
                chk_idxes[i] = st.checkbox(f'{category_list[i]}', value=True)
        st.write("")
    # filter_dict={'category': {"$eq": '02 ì·¨ì—… ì§€ì›'}}
    # filter_dict = {"$or": [{"title": {"$eq": "ë‚´ì¼ì´ë£¸í•™êµ"}}, {"title": {"$eq": "ì¼í•™ìŠµë³‘í–‰ì œ"}}]}
    filter_dict = {}
    chk_num = sum(chk_idxes)
    if chk_num == 0:
        st.warning("ì¹´í…Œê³ ë¦¬ë¥¼ 1ê°œ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”!", icon="âš ï¸")
        return
    elif chk_num == 1:
        for i in range(filter_len):
            if chk_idxes[i]:
                filter_dict["category"] = {"$eq": category_list[i]}
                break
    else:
        filter_dict["$or"] = []
        for i in range(filter_len):
            if chk_idxes[i]:
                filter_dict["$or"].append({"category": {"$eq": category_list[i]}})

    ##### METHOD RESULT CONTAINER
    # invoke container
    invoke_container = st.container()
    with invoke_container:
        st.markdown("### ìƒì„±ëœ ë‹µë³€")
        invoke_empty = st.empty()
        st.divider()
    #retrieve container
    retrieve_container = st.container()
    with retrieve_container:
        st.markdown("### ê´€ë ¨ ë¬¸ì„œ")  
    ##### VECTORSTORE CONFIG
    vectorstore = vectorstore_config()

    ##### stream test
    pipeline = RAGPipeline(vectorstore=vectorstore.vs, embedding=vectorstore.emb, model=model, filter_dict=filter_dict)
    
    #ON Button Event
    if query or search_button:
        invoke_empty.markdown("ì‹¤í–‰ ì¤‘ ... ")
        with ThreadPoolExecutor() as executor:
            future_invoke = executor.submit(run_pipeline_task, query, pipeline.invoke)
            future_retrieve = executor.submit(run_pipeline_task, query, pipeline.retrieve)
            futures = {future_invoke: 'Invoke', future_retrieve: 'Retrieve'}
            # futures = {future_retrieve: 'Retrieve'}

            for future in as_completed(futures):
                task_name = futures[future]
                result, elapsed_time = future.result()

                if task_name == 'Invoke':
                    invoke_empty.empty()
                    invoke_empty.write({"ë‹µë³€ ë‚´ìš©": f'{result} \n\nì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ'})
                    # invoke_empty.markdown(f'{result} \n\n ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ', unsafe_allow_html=True)
                    pass
                else:
                    empty = retrieve_container.empty()
                    relavant_docs_list = RAGPipeline.format_docs(result).split("\n***\n")
                    # empty.markdown(relavant_docs_list, unsafe_allow_html=True)
                    with empty.container():
                        for f_doc in relavant_docs_list:
                            h, c, t = f_doc.split("$$$")
                            st.markdown(h, unsafe_allow_html=True)
                            st.write({"ìƒì„¸ë³´ê¸°": c})
                            st.markdown(t, unsafe_allow_html=True)
                            st.divider()
                        st.write(f'ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ')

            # for t in executor._threads:
            #     add_script_run_ctx(t)
        st.toast('ê²€ìƒ‰ ì™„ë£Œ!', icon="âœ…")

        # progress_text = f'Finding about "{query_text}"...'
        # with st.spinner(progress_text):
        #     placeholder = st.empty()
        #     a = "ê²€ìƒ‰ì¤‘"
        #     placeholder.text(a)
            
        #     ## RAG result
        #     pipeline = RAGPipeline(vectorstore=vectorstore.vs, embedding=vectorstore.emb, model=model)
        #     results_rag = pipeline.invoke(query_text)
        #     a = "ê´€ë ¨ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ. ë‹µë³€ ìƒì„±ì¤‘"
        #     placeholder.text(a)
        #     time.sleep(2)
        #     # semantic_search using vectorstores of langchain
        #     results_vs = pipeline.retrieve(query_text)
        #     a = "ë‹µë³€ ìƒì„± ì™„ë£Œ"
        #     placeholder.text(a)
        #     time.sleep(1)
        #     placeholder.empty()
        #     st.success("ê²€ìƒ‰ ì™„ë£Œ!")
            
        # #Get Answer
        # answer, docs = st.tabs([f"{search_name} ê²°ê³¼", "ê´€ë ¨ ì œë„"])
        # with answer:
        #     st.subheader(f'''
        #                 "{query_text}"ì— ëŒ€í•œ **:blue[{search_name}]** ê²°ê³¼ì…ë‹ˆë‹¤.''')
        #     st.write("")
        #     st.markdown(results_rag)
        # with docs:
        #     st.subheader(f'"{query_text}" ê´€ë ¨ ë³µì§€ ì œë„ì…ë‹ˆë‹¤.')
        #     st.markdown(RAGPipeline.format_docs(results_vs))

#### run -> streamlit run app.py
if __name__ == "__main__":
    main()